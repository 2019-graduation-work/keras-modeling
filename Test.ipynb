{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"bi-LSTM.ipynb\n",
    "\n",
    "Automatically generated by Colaboratory.\n",
    "\n",
    "Original file is located at\n",
    "    https://colab.research.google.com/drive/1Phkwb13SQ9Z4DyecE7qndGBulMKJwfXT\n",
    "\"\"\"\n",
    "sentence = [] #문장\n",
    "category = [] #카테고리\n",
    "vocab_size = 0\n",
    "def read_file(file_name,index):\n",
    "    import re\n",
    "    from konlpy.tag import Okt\n",
    "    global sentence,category\n",
    "    f = open(file_name, \"r\", encoding='utf-8')\n",
    "    list1 = []\n",
    "    okt = Okt()\n",
    "    for i in range(1, 10000):\n",
    "        line = f.readline()  # 한 줄씩 읽음.\n",
    "        if not line: break  # 모두 읽으면 while문 종료.\n",
    "        line = re.sub('[^ㄱ-ㅣ가-힝0-9a-zA-Z\\\\s]', '', line)  # 특수문자제거를 위해 한글과 알파벳만 남기기.\n",
    "        noun = okt.nouns(line)  # 명사추출\n",
    "        stop_words = ['것', '수', '거', '곳', '저', '안', '제', '더', '때', '이', '진짜', '바로', '정말',\n",
    "                      '여기', '개', '분', '정도', '그', '요', '중', '밤', '그', '요', '중', '위', '나', '내',\n",
    "                      '가장', '게', '점', '좀', '또', '달', '말', '해', '은', '향', '번', '날', '아주', '완전', '꼭', '듯',\n",
    "                      '그냥', '조금', '듯', '층', '사실', '도', '뭐', '살', '살짝', '걸', '쪽', '얼', '만', '꽤', '후']\n",
    "        temp = [each_word for each_word in noun if each_word not in stop_words] #불용어 제거\n",
    "        if temp:  # temp에 결과가 존재할 경우에만 (길이 0인거는 제외)\n",
    "            list1.append(temp)\n",
    "    f.close()\n",
    "    sentence = sentence + list1\n",
    "    cate = [index for k in range(len(list1))]\n",
    "    category = category + cate\n",
    "\n",
    "def proceed():\n",
    "    global sentence, category,vocab_size\n",
    "    from keras.preprocessing.text import Tokenizer\n",
    "    t = Tokenizer()\n",
    "    t.fit_on_texts(sentence) #사전 만들기\n",
    "    vocab_size = len(t.word_index) + 1 #단어들의 개수\n",
    "    print(\"vocab_size: \",vocab_size)\n",
    "    X_encoded = t.texts_to_sequences(sentence) # 정수 인코딩\n",
    "\n",
    "    max_len = max(len(l) for l in X_encoded)\n",
    "    print(\"max_len :\", max_len) #단어들의 최대개수\n",
    "\n",
    "    # 모든 문장을 패딩하여 길이 최대 길이로 만들어줍니다.\n",
    "    from keras.preprocessing.sequence import pad_sequences\n",
    "    predData = pad_sequences(X_encoded, maxlen=max_len, padding='post')\n",
    "    #print(predData)\n",
    "\n",
    "    return predData\n",
    "\n",
    "\n",
    "def modeling1():\n",
    "    # MODELING 3\n",
    "    # https://www.tensorflow.org/tutorials/text/text_classification_rnn\n",
    "    global vocab_size\n",
    "    from keras.models import Sequential\n",
    "    from keras.layers import Dense, LSTM, Bidirectional, Embedding\n",
    "    from keras.optimizers import Adam\n",
    "\n",
    "    model = Sequential([\n",
    "        Embedding(vocab_size, 64, mask_zero=True),\n",
    "        Bidirectional(LSTM(64)),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dense(3, activation='softmax') #결과갯수\n",
    "    ])\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer=Adam(1e-4),\n",
    "                  metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "def modeling2():\n",
    "    # MODELING 4\n",
    "    # https://www.tensorflow.org/tutorials/text/text_classification_rnn\n",
    "    global vocab_size\n",
    "    from keras.models import Sequential\n",
    "    from keras.layers import Dense, LSTM, Dropout, Bidirectional, Embedding\n",
    "    from keras.optimizers import Adam\n",
    "\n",
    "    model = Sequential([\n",
    "        Embedding(vocab_size, 64, mask_zero=True),\n",
    "        Bidirectional(LSTM(64, return_sequences=True)),\n",
    "        Bidirectional(LSTM(32)),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dropout(0.5),\n",
    "        Dense(3, activation='softmax') #결과갯수\n",
    "    ])\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer=Adam(1e-4),\n",
    "                  metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#파일 읽기\n",
    "read_file(\"/text/desrt(1006).txt\", 0)\n",
    "read_file(\"/text/food(974).txt\", 1)\n",
    "read_file(\"/text/recipi(997).txt\", 2)\n",
    "#전처리\n",
    "predData = proceed()\n",
    "\n",
    "# train 데이터와 test 데이터로 나누어줍니다.\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, train_labels, test_labels = train_test_split(predData, category, test_size=0.2,random_state=321)  # 8:2 #3,4번째 인자 바꾸어주면됨\n",
    "from keras.utils.np_utils import to_categorical\n",
    "Y_train = to_categorical(train_labels)  # 라벨데이터 원 핫 인코딩(0-9사이의값(?))\n",
    "Y_test = to_categorical(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#모델\n",
    "#model = modeling1() #테스트 정확도: 0.6667\n",
    "model = modeling2() # 테스트 정확도: 0.6941\n",
    "history = model.fit(X_train, Y_train, batch_size=128, epochs=6)\n",
    "#테스트 데이터\n",
    "print(\"\\n 테스트 정확도: %.4f\" % (model.evaluate(X_test, Y_test)[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"/content/drive/My Drive/koLab/trained_model.h5\", include_optimizer=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
